from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.vectorstores import Chroma
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import Ollama

def create_vectorstore(chunks):

    embeddings = HuggingFaceInstructEmbeddings(model_name ="")
    vectorstore = Chroma.from_texts(texts=chunks, embedding=embeddings)

    return vectorstore

def create_conversation_chain(vectorstore):

    llm = Ollama(model="llama3") 
    model_kwargs={"max_length":512, "Temperature":0.1}


    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)


    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm = llm,
        retriever= vectorstore.as_retriever(),
        memory=memory
    )
       
    

    return conversation_chain
